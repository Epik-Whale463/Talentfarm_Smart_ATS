{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import os\n",
    "\n",
    "api_key = \"L9Z3JFdu5LaSollNKnMAJDytfS6i3o5X\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "uploaded_pdf = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"./resumes/RamaCharan_resume.pdf\",\n",
    "        \"content\": open(\"./resumes/RamaCharan_resume.pdf\", \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.retrieve(file_id=uploaded_pdf.id)\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)\n",
    "\n",
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": signed_url.url,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"Rama Charan Pisupati\", \"email\": \"rama.charan.official@gmail.com\", \"phone\": \"+91 9908673209\", \"location\": \"Guntur, India\", \"linkedin\": \"LinkedIn\", \"github\": \"Github\", \"profile\": \"I'm a focused and driven computer science student with a passion for applied AI and machine learning. I take a methodical approach to solving problems and finding efficient solutions. I enjoy connecting theory to practical applications and constantly look for ways to improve performance. I'm eager to take on challenges and learn from experienced mentors.\", \"education\": [{\"degree\": \"B.Tech, Artificial Intelligence and Machine Learning\", \"institution\": \"Vasireddy Venkatadri Institute of Technology\", \"cgpa\": \"8.46/10\", \"relevant_coursework\": [\"Deep Learning\", \"Operating Systems\", \"Data Structures\", \"DBMS\", \"Advanced Python Programming\"]}], \"projects\": [{\"title\": \"Speech Synthesis using Deep Learning, Telugu Text-to-Speech System\", \"description\": \"Developed a production-ready Telugu Text-to-Speech system processing 1000+ unique characters, achieving 40% improved speech naturalness (MOS 4.2/5) through SpeechT5 fine-tuning on 8,500+ samples. Reduced character recognition errors by 78% using custom transliteration with 512-dimensional speaker embeddings, delivering natural-sounding speech with two distinct voice profiles optimized through 8x gradient accumulation.\"}, {\"title\": \"LogFast - High Performance Log Analyzer, C++ Optimization Project\", \"description\": \"Developed a high-performance C++ log analyzer utilizing multithreaded architecture to efficiently generate and parse log files. Implemented producer-consumer patterns with thread pools for optimal CPU utilization, achieving significant throughput improvements through buffered I/O and lock-free operations where possible. Features automatic hardware detection for thread optimization, detailed performance statistics, and comprehensive log level analysis.\"}, {\"title\": \"GitHub Analytics Dashboard Engine, Developer Analytics Platform\", \"description\": \"Created a developer analytics platform with responsive React.js and D3.js visualizations displaying real-time GitHub metrics, achieving 99.9% uptime and 95% user satisfaction among 50+ beta testers. Integrated web development best practices and deployed on Vercel to ensure scalable performance, robust interactivity, and seamless presentation of complex developer insights.\"}, {\"title\": \"College Data Query System, Generative AI\", \"description\": \"Developed an interactive AI chatbot using Retrieval-Augmented Generation (RAG) that combines open-source LLMs with Qdrant for vector storage and Nvidia NIM's Snowflake embedding model, written primarily in Python. Deployed on Oracle Cloud to handle over 100 concurrent users, this solution streamlines complex data queries while significantly enhancing response accuracy and reducing latency.\"}], \"skills\": {\"programming_languages\": [\"Python\", \"C++\", \"SQL\", \"BASH\"], \"frameworks_libraries\": [\"Scikit-learn\", \"TensorFlow\", \"LangChain\", \"LlamaIndex\"], \"data_science_ai\": [\"Machine Learning\", \"Supervised and Unsupervised Learning\", \"Natural Language Processing (NLP)\", \"Generative AI\", \"Retrieval-Augmented Generation\", \"Transformers\", \"Large Language Models (LLM)\", \"Neural Networks\"], \"databases\": [\"MySQL\", \"MongoDB\", \"PostgreSQL\", \"Qdrant\"], \"cloud_devops\": [\"AWS (EC2, S3)\", \"Docker\"], \"tools\": [\"Git\", \"GitHub\", \"Linux\", \"Data Visualization (Matplotlib, Seaborn, Tableau)\"]}, \"certificates\": [{\"title\": \"AWS Certified AI Practitioner\", \"description\": \"Gained foundational knowledge of AWS AI services (SageMaker, Bedrock).\"}, {\"title\": \"NPTEL Data Science using Python\", \"description\": \"Covered Python fundamentals, libraries (Pandas, NumPy) and basic ML algorithms.\"}], \"organizations_positions\": [{\"organization\": \"AI-HUB\", \"position\": \"Mentor\", \"duration\": \"2025 - present\", \"location\": \"Guntur, India\", \"responsibilities\": [\"Leads knowledge-sharing sessions for 1st-3rd year students, covering AI/ML concepts, hands-on projects, and coding best practices\"]}, {\"organization\": \"Google Developers Group Nagpur\", \"position\": \"Member\", \"duration\": \"2024 - present\", \"location\": \"Nagpur, India\", \"responsibilities\": [\"Attended Build with AI Event, fostering networking, connections, and hands-on learning in AI innovations.\"]}]}]\n"
     ]
    }
   ],
   "source": [
    "# Specify model\n",
    "model = \"mistral-small-latest\"\n",
    "\n",
    "# Initialize the Mistral client\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "# Define the messages for the chat\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Structure every single detail into JSON response and output a structured JSON response.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": signed_url.url\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "chat_response = client.chat.complete(\n",
    "      model = model,\n",
    "      messages = messages,\n",
    "      response_format = {\n",
    "          \"type\": \"json_object\",\n",
    "      }\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 0 ---\n",
      "# Rama Charan Pisupati \n",
      "\n",
      "Aspiring AI/ML Engineer\n",
      "rama.charan.official@gmail.com | +91 9908673209 | Guntur, India | LinkedIn | Github\n",
      "\n",
      "## PROFILE\n",
      "\n",
      "I'm a focused and driven computer science student with a passion for applied AI and machine learning. I take a methodical approach to solving problems and finding efficient solutions. I enjoy connecting theory to practical applications and constantly look for ways to improve performance. I'm eager to take on challenges and learn from experienced mentors.\n",
      "\n",
      "## EDUCATION\n",
      "\n",
      "B.Tech, Artificial Intelligence and Machine Learning,\n",
      "\n",
      "Vasireddy Venkatadri Institute of Technology\n",
      "\n",
      "- CGPA: 8.46/10\n",
      "- Relevant Coursework: Deep Learning, Operating Systems, Data Structures, DBMS, Advaned Python Programming\n",
      "\n",
      "\n",
      "## PROJECTS\n",
      "\n",
      "Speech Synthesis using Deep Learning, Telugu Text-to-Speech System $\\square$ - Developed a production-ready Telugu Text-to-Speech system processing 1000+ unique characters, achieving $40 \\%$ improved speech naturalness (MOS 4.2/5) through SpeechT5 fine-tuning on 8,500+ samples. Reduced character recognition errors by $78 \\%$ using custom transliteration with 512-dimensional speaker embeddings, delivering natural-sounding speech with two distinct voice profiles optimized through 8x gradient accumulation.\n",
      "LogFast - High Performance Log Analyzer, C++ Optimization Project $\\square$ - Developed a high-performance $\\mathrm{C}++$ log analyzer utilizing multithreaded architecture to efficiently generate and parse log files. Implemented producer-consumer patterns with thread pools for optimal CPU utilization, achieving significant throughput improvements through buffered I/O and lock-free operations where possible. Features automatic hardware detection for thread optimization, detailed performance statistics, and comprehensive log level analysis.\n",
      "GitHub Analytics Dashboard Engine, Developer Analytics Platform $\\square$ - Created a developer analytics platform with responsive React.js and D3.js visualizations displaying real-time GitHub metrics, achieving $99.9 \\%$ uptime and $95 \\%$ user satisfaction among $50+$ beta testers. Integrated web development best practices and deployed on Vercel to ensure scalable performance, robust interactivity, and seamless presentation of complex developer insights.\n",
      "College Data Query System, Generative AI $\\square$\n",
      "- Developed an interactive AI chatbot using Retrieval-Augmented Generation (RAG) that combines open-source LLMs with Qdrant for vector storage and Nvidia NIM's Snowflake embedding model, written primarily in Python.\n",
      "- Deployed on Oracle Cloud to handle over 100 concurrent users, this solution streamlines complex data queries while significantly enhancing response accuracy and reducing latency.\n",
      "\n",
      "\n",
      "## SKILLS\n",
      "\n",
      "Programming Languages: Python | C++ | SQL | BASH\n",
      "Frameworks \\& Libraries: Scikit-learn | TensorFlow | LangChain | LlamaIndex\n",
      "Data Science \\& AI: Machine Learning | Supervised and Unsupervised Learning | Natural Language Processing (NLP) | GenerativeAI | Retrieval-Augmented Generation | Transformers | Large Language Models (LLM) | Neural Networks\n",
      "Databases: MySQL, | MongoDB | PostgreSQL | Qdrant\n",
      "Cloud \\& DevOps: AWS (EC2, S3) | Docker\n",
      "Tools: Git, GitHub, | Linux | Data Visualization (Matplotlib, Seaborn, Tableau)\n",
      "\n",
      "## CERTIFICATES\n",
      "\n",
      "AWS Certified AI Practitioner\n",
      "Gained foundational knowledge of AWS AI services (SageMaker, Bedrock).\n",
      "NPTEL Data Science using Python $\\square$\n",
      "Covered Python fundamentals, libraries (Pandas, NumPy) and basic ML algorithms.\n",
      "\n",
      "## ORGANISATIONS AND POSITIONS\n",
      "\n",
      "AI-HUB, Mentor $\\square$\n",
      "2025 - present | Guntur, India\n",
      "\n",
      "- Leads knowledge-sharing sessions for 1st-3rd year students, covering AI/ML concepts, hands-on projects, and coding best practices\n",
      "Google Developers Group Nagpur, Member $\\square$\n",
      "2024 - present | Nagpur, India\n",
      "- Attended Build with AI Event, fostering networking, connections, and hands- on learning in AI innovations.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For multiple pages, you could do:\n",
    "for page in ocr_response.pages:\n",
    "    print(f\"--- Page {page.index} ---\")\n",
    "    print(page.markdown)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral OCR AI Implementation Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the implementation and usage of Mistral's OCR (Optical Character Recognition) AI capabilities for processing PDF documents, specifically resumes. The implementation follows a multi-step approach to extract structured data from documents.\n",
    "\n",
    "## Implementation Breakdown\n",
    "\n",
    "### 1. **Client Initialization**\n",
    "```python\n",
    "from mistralai import Mistral\n",
    "client = Mistral(api_key=api_key)\n",
    "```\n",
    "- Uses the new Mistral SDK (v1.0+)\n",
    "- Requires valid API key for authentication\n",
    "- Creates a client instance for all subsequent operations\n",
    "\n",
    "### 2. **File Upload Process**\n",
    "```python\n",
    "uploaded_pdf = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"./resumes/RamaCharan_resume.pdf\",\n",
    "        \"content\": open(\"./resumes/RamaCharan_resume.pdf\", \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")\n",
    "```\n",
    "**Key Points:**\n",
    "- Uploads PDF to Mistral's servers for processing\n",
    "- Purpose is set to \"ocr\" indicating OCR processing intent\n",
    "- Returns file object with unique ID for subsequent operations\n",
    "- File content is read in binary mode (\"rb\")\n",
    "\n",
    "### 3. **Signed URL Generation**\n",
    "```python\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)\n",
    "```\n",
    "**Purpose:**\n",
    "- Creates a secure, temporary URL for accessing the uploaded file\n",
    "- Required for OCR processing and chat completion\n",
    "- Provides secure access without exposing API keys\n",
    "\n",
    "### 4. **OCR Processing**\n",
    "```python\n",
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": signed_url.url,\n",
    "    }\n",
    ")\n",
    "```\n",
    "**Features:**\n",
    "- Uses latest OCR model for best accuracy\n",
    "- Processes document via URL reference\n",
    "- Returns structured OCR response with page-level data\n",
    "- Extracts text in markdown format for better structure preservation\n",
    "\n",
    "### 5. **Chat-Based Structured Extraction**\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Structure every single detail into JSON response and output a structured JSON response.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": signed_url.url\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "chat_response = client.chat.complete(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=messages,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "```\n",
    "**Advanced Features:**\n",
    "- Multimodal input (text + document)\n",
    "- Direct document URL processing in chat\n",
    "- Forced JSON output format\n",
    "- Combines OCR with LLM understanding for intelligent extraction\n",
    "\n",
    "## Strengths of This Approach\n",
    "\n",
    "### ✅ **Advantages:**\n",
    "1. **High Accuracy**: Mistral's OCR model is specifically trained for document understanding\n",
    "2. **Structured Output**: Direct JSON formatting reduces post-processing needs\n",
    "3. **Multimodal Processing**: Combines visual document understanding with text analysis\n",
    "4. **Page-Level Access**: Can process multi-page documents with page-specific extraction\n",
    "5. **Secure Processing**: Uses signed URLs for secure document handling\n",
    "6. **Modern SDK**: Uses latest Mistral SDK with up-to-date features\n",
    "\n",
    "### ✅ **Use Cases:**\n",
    "- Resume parsing and ATS integration\n",
    "- Document digitization\n",
    "- Form data extraction\n",
    "- Invoice/receipt processing\n",
    "- Legal document analysis\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "### ⚠️ **Limitations:**\n",
    "1. **API Dependency**: Requires internet connection and API availability\n",
    "2. **Cost**: Each OCR and chat completion call incurs API costs\n",
    "3. **File Upload**: Documents must be uploaded to Mistral servers\n",
    "4. **Rate Limits**: Subject to API rate limiting\n",
    "5. **File Size**: May have limits on document size and page count\n",
    "\n",
    "### ⚠️ **Security Considerations:**\n",
    "- Documents are uploaded to external servers\n",
    "- Ensure compliance with data privacy regulations\n",
    "- Consider data retention policies\n",
    "- Use appropriate access controls for sensitive documents\n",
    "\n",
    "## Performance Optimization Tips\n",
    "\n",
    "### 🚀 **Best Practices:**\n",
    "1. **Batch Processing**: Process multiple documents in batches when possible\n",
    "2. **Caching**: Cache OCR results for repeated processing\n",
    "3. **Error Handling**: Implement robust error handling for API failures\n",
    "4. **Retry Logic**: Add exponential backoff for transient failures\n",
    "5. **Input Validation**: Validate file types and sizes before upload\n",
    "\n",
    "## Integration with ATS Backend\n",
    "\n",
    "The implementation in this notebook directly addresses the resume parsing challenges identified in the ATS project:\n",
    "\n",
    "1. **Replaces Deprecated Methods**: Uses new SDK instead of deprecated file upload methods\n",
    "2. **Improved Accuracy**: OCR + LLM provides better extraction than text-only parsing\n",
    "3. **Structured Data**: Direct JSON output fits ATS database schema requirements\n",
    "4. **Scalable Architecture**: Can be integrated into the Flask backend for production use\n",
    "\n",
    "## Recommended Next Steps\n",
    "\n",
    "1. **Error Handling**: Add comprehensive error handling for production use\n",
    "2. **Data Validation**: Implement validation for extracted JSON data\n",
    "3. **Backend Integration**: Integrate this approach into the Flask resume parser service\n",
    "4. **Performance Monitoring**: Add logging and monitoring for OCR operations\n",
    "5. **Cost Optimization**: Implement caching and rate limiting strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: OCR vs Current Backend Approach\n",
    "\n",
    "print(\"=== COMPARISON: Mistral OCR vs Current Backend ===\\n\")\n",
    "\n",
    "# Current backend approach (PyPDF2 + python-docx)\n",
    "print(\"🔄 Current Backend Approach:\")\n",
    "print(\"- Uses PyPDF2 for PDF text extraction\")\n",
    "print(\"- Uses python-docx for DOCX text extraction\")\n",
    "print(\"- Sends extracted text to Mistral chat completion\")\n",
    "print(\"- Limited to text-extractable content only\")\n",
    "print(\"- May miss formatting, tables, or image-based content\\n\")\n",
    "\n",
    "# OCR approach demonstrated here\n",
    "print(\"🚀 Mistral OCR Approach:\")\n",
    "print(\"- Processes visual content of documents\")\n",
    "print(\"- Handles scanned PDFs and image-based content\")\n",
    "print(\"- Preserves document structure and formatting\")\n",
    "print(\"- Direct document understanding without intermediate text extraction\")\n",
    "print(\"- More accurate for complex layouts\\n\")\n",
    "\n",
    "print(\"💡 Key Differences:\")\n",
    "print(\"1. OCR can handle scanned documents and images\")\n",
    "print(\"2. OCR preserves visual layout and structure\")\n",
    "print(\"3. OCR is more accurate for complex formatting\")\n",
    "print(\"4. OCR incurs additional API costs\")\n",
    "print(\"5. OCR requires document upload to Mistral servers\\n\")\n",
    "\n",
    "print(\"🎯 Recommendation:\")\n",
    "print(\"- Use OCR for scanned documents or complex layouts\")\n",
    "print(\"- Use text extraction for simple, text-based documents\")\n",
    "print(\"- Implement hybrid approach based on document characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend Integration Example\n",
    "\n",
    "class MistralOCRParser:\n",
    "    \"\"\"Enhanced resume parser using Mistral OCR for better document understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.client = Mistral(api_key=api_key)\n",
    "    \n",
    "    def parse_resume_with_ocr(self, file_path: str) -> dict:\n",
    "        \"\"\"Parse resume using OCR for enhanced accuracy\"\"\"\n",
    "        try:\n",
    "            # Step 1: Upload document\n",
    "            with open(file_path, 'rb') as file:\n",
    "                uploaded_file = self.client.files.upload(\n",
    "                    file={\n",
    "                        \"file_name\": file_path.split('/')[-1],\n",
    "                        \"content\": file,\n",
    "                    },\n",
    "                    purpose=\"ocr\"\n",
    "                )\n",
    "            \n",
    "            # Step 2: Get signed URL\n",
    "            signed_url = self.client.files.get_signed_url(file_id=uploaded_file.id)\n",
    "            \n",
    "            # Step 3: Extract structured data using chat completion\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"\"\"Extract and structure all resume information into JSON format:\n",
    "                            {\n",
    "                                \"personal_info\": {\"name\": \"\", \"email\": \"\", \"phone\": \"\", \"location\": \"\"},\n",
    "                                \"summary\": \"\",\n",
    "                                \"work_experience\": [{\"company\": \"\", \"title\": \"\", \"start_date\": \"\", \"end_date\": \"\", \"description\": []}],\n",
    "                                \"education\": [{\"institution\": \"\", \"degree\": \"\", \"start_date\": \"\", \"end_date\": \"\"}],\n",
    "                                \"skills\": {\"technical\": [], \"tools\": [], \"soft\": []},\n",
    "                                \"projects\": [{\"name\": \"\", \"description\": \"\", \"technologies\": []}],\n",
    "                                \"certifications\": [{\"name\": \"\", \"issuer\": \"\", \"date\": \"\"}],\n",
    "                                \"achievements\": []\n",
    "                            }\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"document_url\", \n",
    "                            \"document_url\": signed_url.url\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            response = self.client.chat.complete(\n",
    "                model=\"mistral-small-latest\",\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            # Parse JSON response\n",
    "            import json\n",
    "            structured_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Cleanup: Delete uploaded file (optional)\n",
    "            # self.client.files.delete(file_id=uploaded_file.id)\n",
    "            \n",
    "            return structured_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"OCR parsing failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "print(\"🔧 Backend Integration Pattern:\")\n",
    "print(\"1. Create MistralOCRParser instance in Flask app\")\n",
    "print(\"2. Use for complex documents or when text extraction fails\")\n",
    "print(\"3. Fallback to text-based extraction for simple documents\")\n",
    "print(\"4. Cache results to minimize API calls\")\n",
    "print(\"5. Implement error handling and retry logic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR QUALITY ANALYSIS ===\n",
      "\n",
      "📄 OCR Response Analysis:\n",
      "- Number of pages processed: 1\n",
      "- Page 1: 3,902 characters extracted\n",
      "- Total characters extracted: 3,902\n",
      "- Average characters per page: 3,902\n",
      "\n",
      "🧠 Structured Extraction Analysis:\n",
      "- Total extractable fields: 63\n",
      "- Populated fields: 63\n",
      "- Completion rate: 100.0%\n",
      "\n",
      "📊 Section Analysis:\n",
      "\n",
      "✨ Overall Quality Score: 100.0%\n",
      "🎉 Excellent extraction quality!\n"
     ]
    }
   ],
   "source": [
    "# OCR Quality Analysis\n",
    "\n",
    "def analyze_ocr_quality(ocr_response, chat_response):\n",
    "    \"\"\"Analyze the quality and completeness of OCR extraction\"\"\"\n",
    "    \n",
    "    print(\"=== OCR QUALITY ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Analyze OCR response structure\n",
    "    print(\"📄 OCR Response Analysis:\")\n",
    "    print(f\"- Number of pages processed: {len(ocr_response.pages)}\")\n",
    "    \n",
    "    total_chars = 0\n",
    "    for i, page in enumerate(ocr_response.pages):\n",
    "        page_chars = len(page.markdown)\n",
    "        total_chars += page_chars\n",
    "        print(f\"- Page {i+1}: {page_chars:,} characters extracted\")\n",
    "    \n",
    "    print(f\"- Total characters extracted: {total_chars:,}\")\n",
    "    print(f\"- Average characters per page: {total_chars // len(ocr_response.pages):,}\\n\")\n",
    "    \n",
    "    # Analyze structured output\n",
    "    print(\"🧠 Structured Extraction Analysis:\")\n",
    "    try:\n",
    "        import json\n",
    "        structured_data = json.loads(chat_response.choices[0].message.content)\n",
    "        \n",
    "        # Count populated fields\n",
    "        def count_fields(data, prefix=\"\"):\n",
    "            count = 0\n",
    "            populated = 0\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, (dict, list)):\n",
    "                        sub_count, sub_populated = count_fields(value, f\"{prefix}.{key}\")\n",
    "                        count += sub_count\n",
    "                        populated += sub_populated\n",
    "                    else:\n",
    "                        count += 1\n",
    "                        if value and str(value).strip():\n",
    "                            populated += 1\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if isinstance(item, (dict, list)):\n",
    "                        sub_count, sub_populated = count_fields(item, prefix)\n",
    "                        count += sub_count\n",
    "                        populated += sub_populated\n",
    "                    else:\n",
    "                        count += 1\n",
    "                        if item and str(item).strip():\n",
    "                            populated += 1\n",
    "            \n",
    "            return count, populated\n",
    "        \n",
    "        total_fields, populated_fields = count_fields(structured_data)\n",
    "        completion_rate = (populated_fields / total_fields * 100) if total_fields > 0 else 0\n",
    "        \n",
    "        print(f\"- Total extractable fields: {total_fields}\")\n",
    "        print(f\"- Populated fields: {populated_fields}\")\n",
    "        print(f\"- Completion rate: {completion_rate:.1f}%\")\n",
    "        \n",
    "        # Analyze specific sections\n",
    "        sections = ['personal_info', 'work_experience', 'education', 'skills', 'projects']\n",
    "        print(\"\\n📊 Section Analysis:\")\n",
    "        for section in sections:\n",
    "            if section in structured_data:\n",
    "                data = structured_data[section]\n",
    "                if isinstance(data, list):\n",
    "                    print(f\"- {section}: {len(data)} items\")\n",
    "                elif isinstance(data, dict):\n",
    "                    filled = sum(1 for v in data.values() if v and str(v).strip())\n",
    "                    print(f\"- {section}: {filled}/{len(data)} fields filled\")\n",
    "                else:\n",
    "                    status = \"✓ Present\" if data and str(data).strip() else \"✗ Missing\"\n",
    "                    print(f\"- {section}: {status}\")\n",
    "        \n",
    "        print(f\"\\n✨ Overall Quality Score: {completion_rate:.1f}%\")\n",
    "        \n",
    "        if completion_rate >= 80:\n",
    "            print(\"🎉 Excellent extraction quality!\")\n",
    "        elif completion_rate >= 60:\n",
    "            print(\"👍 Good extraction quality\")\n",
    "        elif completion_rate >= 40:\n",
    "            print(\"⚠️ Moderate extraction quality - may need manual review\")\n",
    "        else:\n",
    "            print(\"❌ Poor extraction quality - manual processing recommended\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing structured data: {str(e)}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_ocr_quality(ocr_response, chat_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
